# -*- coding: utf-8 -*-
"""Copie de YELLOOOOW STAGE CLEAN script not annotated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18JtRm085lU0vYBkja95dUmGNwCQTAWER

# ACTUAL CODEEEE let's convert the annotations to csv normal ones

Southern yellow cheeked crested gibbons BUT IT IS THE SAME FOR ALL

## LOADING
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
#USING RNN for pattern identification.

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import librosa
from scipy.signal import spectrogram
from tqdm.notebook import tqdm
import csv
from scipy.optimize import minimize_scalar
import plotly.graph_objects as go
import os #for iteration of the files from one folder
from os import listdir
from scipy import signal
from scipy.fft import fftshift
import matplotlib.pyplot as plt
import pymc as pm
import numpy as np
import pandas as pd

# notes_grey = pd.read_csv("/content/drive/MyDrive/internship/northern_grey_notes.csv", sep=',')
# # sound =
# notes_grey.columns = ["file", "time", "f0"]

"""CODES of the individuals NORTHERN GREY GIBBONS by order :
- M9_20190823_044002
- M9_20190823_052003
- M10_20190824_044002
- M10_20190824_052003
- M11_20190825_044002
- M11_20190825_052003

"""

# We have 6 audio files. (6 individuals) NORTHERN GREY Gibbons
audiogrey1 =  ('/content/drive/My Drive/gibbons/Dena/YELLO/R1048_WA_20220812_050002.wav') # M9_20190823_044002.wav
audiogrey2 =  ('/content/drive/MyDrive/gibbons/Dena/YELLO/R1048_WA_20220901_060002.wav') # M9_20190823_052003.wav
audiogrey3 =  ('/content/drive/MyDrive/gibbons/Dena/YELLO/R1049_WA_20221009_060002.wav') # M10_20190824_044002.wav
audiogrey4 =  ('/content/drive/MyDrive/gibbons/Dena/YELLO/R1049_WA_20221016_060002.wav') # M10_20190824_052003.wav
audiogrey5 =  ('/content/drive/MyDrive/gibbons/Dena/YELLO/R1049_WA_20221022_050002.wav') # M11_20190825_044002.wav

audiogrey1

"""ANNOTATION STORED HERE"""

grey1



angrey1 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/M9_20190823_044002.Table.1.selections.txt", sep='\t') # M9_20190823_044002
grey1 = angrey1[['Begin Time (s)','End Time (s)']]
grey1.to_csv('/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/annotationme/M9_20190823_044002.txt', sep='\t', index=False, header=["beg","end"])

angrey2 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/M9_20190823_052003.Table.1.selections.txt", sep='\t') # M9_20190823_052003
grey2 = angrey2[['Begin Time (s)','End Time (s)']]
grey2.to_csv('/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/annotationme/M9_20190823_052003.txt', sep='\t', index=False, header=["beg","end"])

angrey3 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/M10_20190824_044002.Table.1.selections.txt", sep='\t') # M10_20190824_044002
grey3 = angrey3[['Begin Time (s)','End Time (s)']]
grey3.to_csv('/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/annotationme/M10_20190824_044002.txt', sep='\t', index=False, header=["beg","end"])

angrey4 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/M10_20190824_052003.Table.1.selections.txt", sep='\t') # M10_20190824_052003
grey4 = angrey4[['Begin Time (s)','End Time (s)']]
grey4.to_csv('/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/annotationme/M10_20190824_052003.txt', sep='\t', index=False, header=["beg","end"])

angrey5 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/M11_20190825_044002.Table.1.selections.txt", sep='\t') # M11_20190825_044002
grey5 = angrey5[['Begin Time (s)','End Time (s)']]
grey5.to_csv('/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/annotationme/M11_20190825_044002.txt', sep='\t', index=False, header=["beg","end"])

angrey6 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/M11_20190825_052003.Table.1.selections.txt", sep='\t') # M11_20190825_052003
grey6 = angrey6[['Begin Time (s)','End Time (s)']]
grey6.to_csv('/content/drive/MyDrive/gibbons/Dena/nothern grey gibbons/annotationme/M11_20190825_052003.txt', sep='\t', index=False, header=["beg","end"])

"""CODES of the individuals NORTHERN GREY GIBBONS by order :
- M9_20190823_044002
- M9_20190823_052003
- M10_20190824_044002
- M10_20190824_052003
- M11_20190825_044002
- M11_20190825_052003
"""

grey1 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/YELLO/R1048_WA_20220812_050002INDIV1.txt", sep='\t', names=['Begin Time (s)','End Time (s)', "label"])
grey2 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/YELLO/R1048_WA_20220901_060002INDIV2.txt", sep='\t', names=['Begin Time (s)','End Time (s)', "label"]) # M9_20190823_044002
grey3 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/YELLO/R1049_WA_20221009_060002INDIV3.txt", sep='\t', names=['Begin Time (s)','End Time (s)', "label"]) # M9_20190823_044002
grey4 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/YELLO/R1049_WA_20221016_060002INDIV4.txt", sep='\t', names=['Begin Time (s)','End Time (s)', "label"]) # M9_20190823_044002
grey5 = pd.read_csv("/content/drive/MyDrive/gibbons/Dena/YELLO/R1049_WA_20221022_050002INDIV5.txt", sep='\t', names=['Begin Time (s)','End Time (s)', "label"]) # M9_20190823_044002

grey1f= grey1[grey1["label"] == "f"]
grey1m =grey1[grey1["label"] == "m"]
grey2f= grey2[grey2["label"] == "f"]
grey2m = grey2[grey2["label"] == "m"]
grey3f =grey3[grey3["label"] == "f"]
grey3m =grey3[grey3["label"] == "m"]
grey4f =grey4[grey4["label"] == "f"]
grey4m=grey5[grey5["label"] == "m"]
grey5f =grey5[grey5["label"] == "f"]
grey5m=grey5[grey5["label"] == "m"]

"""F0 STORED HERE"""

data1 = pd.read_csv('/content/drive/MyDrive/gibbons/Dena/YELLO/R1048_WA_20220812_050002_f0.csv', sep=",", header=0) # M9_20190823_044002
data2 = pd.read_csv('/content/drive/MyDrive/gibbons/Dena/YELLO/R1048_WA_20220901_060002_f0.csv', sep=",", header=0)# M9_20190823_052003
data3 = pd.read_csv('/content/drive/MyDrive/gibbons/Dena/YELLO/R1049_WA_20221009_060002_f0.csv', sep=",", header=0)# M10_20190824_044002
data4 = pd.read_csv('/content/drive/MyDrive/gibbons/Dena/YELLO/R1048_WA_20220812_050002_f0.csv', sep=",", header=0) # M10_20190824_052003
data5 = pd.read_csv('/content/drive/MyDrive/gibbons/Dena/YELLO/R1049_WA_20221016_060002_f0.csv', sep=",", header=0)# M11_20190825_044002
data6 = pd.read_csv('/content/drive/MyDrive/gibbons/Dena/YELLO/R1049_WA_20221022_050002_f0.csv', sep=",", header=0)# M11_20190825_052003

data1

"""FROM F0s cnn"""

# grey1f0all = data1[['time', 'f0', 'confidence']]
# grey1f0CON = grey1f0all[grey1f0all['confidence']> 0.02]
# grey1f0 = grey1f0CON[['time', 'f0']]

# grey1f0CON

"""SELECTED DATA HERE WITHOUT ANNOTATIONS"""

grey1f0alll = data1[['time', 'f0', 'confidence']]
grey1f0all = grey1f0alll[grey1f0alll['confidence'] > 0.002]
grey1f0 = grey1f0all[["time", "f0"]]# now it is working for all manually cause my loops suck !
grey2f0alll = data2[["time", "f0", "confidence"]]
grey2f0all = grey2f0alll[grey2f0alll['confidence'] > 0.002]
grey2f0 = grey2f0all[["time", "f0"]] #stop here
grey3f0alll = data3[["time", "f0", "confidence"]]
grey3f0all = grey3f0alll[grey3f0alll['confidence'] > 0.002]
grey3f0 = grey3f0all[["time", "f0"]] #stop here
grey4f0alll = data4[["time", "f0", "confidence"]]
grey4f0all = grey4f0alll[grey4f0alll['confidence'] > 0.002]
grey4f0 = grey4f0all[["time", "f0"]] #stop here
grey5f0alll = data5[["time", "f0", "confidence"]]
grey5f0all = grey5f0alll[grey5f0alll['confidence'] > 0.002]
grey5f0 = grey5f0all[["time", "f0"]] #stop here
# grey6f0alll = data6[["time", "f0", "confidence"]]
# grey6f0all = grey6f0alll[grey6f0alll['confidence'] > 0.002]
# grey6f0 = grey6f0all[["time", "f0"]] #stop here

# greyf0 = [grey1f0, grey2f0, grey3f0, grey4f0, grey5f0, grey6f0]
# for idx, i in grey :
#   name = f"grey{idx + 1}f0"
#   greyf0all[name] = i[['time', 'f0', 'confidence']].copy()
#   grey1f0CON[name]  = grey1f0all[f"{i}][grey1f0all[f"{i}] ['confidence']> 0.02].copy()
#   grey1f0final[f"{i}]  = grey1f0CON[f"{i}] [['time', 'f0']]

# this is gonna be for later

"""FROM SALIENCE to test"""

grey1s = data1[["time", "f0", "salience", "confidence"]] # M9_20190823_044002
grey2s = data2[["time",  "f0", "salience","confidence"]] # M9_20190823_052003
grey3s = data3[["time",  "f0", "salience","confidence"]] # M10_20190824_044002
grey4s = data4[["time",  "f0", "salience","confidence"]] # M10_20190824_052003
grey5s = data5[["time",  "f0", "salience","confidence"]] # M11_20190825_044002
grey6s = data6[["time",  "f0", "salience","confidence"]] # M11_20190825_052003

grey1ss = grey1s[grey1s['confidence'] > 0.002]
grey2ss = grey2s[grey2s['confidence'] > 0.002]
grey3ss = grey3s[grey3s['confidence'] > 0.002]
grey4ss = grey4s[grey4s['confidence'] > 0.002]
grey5ss = grey5s[grey5s['confidence'] > 0.002]
grey6ss = grey6s[grey6s['confidence'] > 0.002]

grey1f0

""" # COMPUTE ENERGY"""

#Load full audio
#grey1f0 = the f0s of individual (number = code)
#grey1 = has the annotation (time code and end for each segment of vocalization)

sig, fs = librosa.load(audiogrey3, sr=None)
f,t, Sxx = spectrogram(sig, fs=fs, nperseg=2048, noverlap=1500) #compute the spectrogram here
#convert to dB scale (energy)
Sxx_dB = 10 * np.log10(Sxx + 1e-10)

grey3f0['energy'] = np.nan #to create an empty column

#extract energy at each f0 point of interest yeey
for idx, row in grey3f0.iterrows():
  time_point = row['time']
  freq_point = row['f0']

  idx_col = np.argmin(np.abs(t - time_point))
  idx_row = np.argmin(np.abs(f - freq_point))

  row_start = max(idx_row - 5, 0)
  row_end = min(idx_row + 5, Sxx_dB.shape[0]) #this was agreed with paul, u have to add +5 -5 to land on the correct point yey

  grey3f0.loc[idx, 'energy'] = np.max(Sxx_dB[row_start:row_end, idx_col])

grey1f0

data_final3 = []

for i, x in grey3.iterrows(): # I did this so that I can itterate through the time window
  start, end = x["Begin Time (s)"], x["End Time (s)"]
  #relevant f0 points
  f0_relevant = grey3f0[(grey3f0["time"] >= start) & (grey3f0["time"] <= end)]
  if f0_relevant.empty:
    continue

  max_row = f0_relevant.loc[f0_relevant["energy"].idxmax()]

  data_final3.append({
      "ID" :i,
      "start" : start,
      "end" : end,
      "f0_at_max_energy" : max_row["f0"],
      "time_of_max_energy": max_row["time"]

  })

data_final3 = pd.DataFrame(data_final3)

data_final3.to_csv("/content/drive/MyDrive/gibbons/Dena/YELLO/notesindivyellow3.csv", sep = "\t", index=False) #save because I am running each individually (cause it takes forever in loops and I wanna make sure they all are correct)

data_final1

"""#HARMONICITY TEST OLS"""

from scipy.optimize import minimize_scalar
#SELECT the max energy f0 values omg
f0s = data_final3['f0_at_max_energy'].dropna().values #no need to drop cause pretty sure there arent any but just ot make sure :)

# mse yeeey computing it HERE
def compute_log_mse(fi, f0s, max_harmonics=16):
    harmonics = np.array([fi * i for i in range(1, max_harmonics + 1)])
    mse = np.mean([np.min((f0 - harmonics)**2) for f0 in f0s])
    return mse


result = minimize_scalar(compute_log_mse, bounds=(20, 500), args=(f0s,), method='bounded')
best_f0 = result.x #results stored here
best_mse = result.fun


log_f0s = np.log(f0s)
mu, sigma = np.mean(log_f0s), np.std(log_f0s)

random_mses = []
for _ in range(1000):
    random_sample = np.clip(np.random.lognormal(mean=mu, sigma=sigma, size=len(f0s)), 0, 2000)
    res = minimize_scalar(compute_log_mse, bounds=(20, 500), args=(random_sample,), method='bounded')
    random_mses.append(res.fun)

p_value = np.mean(np.array(random_mses) <= best_mse)

print(f" Best base f₀ (harmonic fit): {best_f0:.2f} Hz")
print(f" p-value: {p_value:.4f}")
print(" V Harmonic!" if p_value < 0.05 else " x Not harmonic.")

results = {
    "f0s": list(f0s),
    "random_mses": list(random_mses),
}

df = pd.DataFrame.from_dict(results, orient="index").T
df["best_f0"] = best_f0
df["best_mse"] = best_mse
df["p_value"] = p_value

df.to_csv("/content/drive/MyDrive/gibbons/Dena/YELLO/harmonic_resultsyellow3.csv", index=False)
 #save the results cause I do not need to run it everytime

"""#Bayesian"""

#Better thant he classsical method I used ! aghhhI am  such a beginner

files = [
    '/content/drive/MyDrive/gibbons/Dena/YELLO/notesindivyellow1.csv',
    '/content/drive/MyDrive/gibbons/Dena/YELLO/notesindivyellow2.csv',
    '/content/drive/MyDrive/gibbons/Dena/YELLO/notesindivyellow3.csv',
    '/content/drive/MyDrive/gibbons/Dena/YELLO/notesindivyellow4.csv',
    '/content/drive/MyDrive/gibbons/Dena/YELLO/notesindivyellow5.csv',
    ]

results = {}  #this is to store them later on

M = 16        # maximum integer multiple
sigma = 0.014 # THIS MY FRIEND, is the jitter!

for i, fpath in enumerate(files, 1):
    data = pd.read_csv(fpath, sep="\t", header=0)
    f_obs = data['f0_at_max_energy'].dropna().values

    # ok, I downsampled cause I coudn't run with the memory I have :( it takes too long ! absolutely too long
    if len(f_obs) > 300:
        f_obs = np.random.choice(f_obs, size=300, replace=False)

    print(f"\nProcessing Individual {i} ({fpath.split('/')[-1]})")

    with pm.Model() as model:

        b = pm.Uniform("b", lower=20, upper=500) #base frequency threshhold ! this is the same as OLS

        #The math lax it must follow
        nk = pm.Categorical("nk", p=np.ones(M)/M, shape=len(f_obs))

        #Probab (predictions of ntoes)
        f_pred = b * (nk + 1)

        #likely
        f_obs_likelihood = pm.Lognormal(
            "f_obs", mu=np.log(f_pred), sigma=sigma, observed=f_obs
        )

        #Thid IS THE POSTERIOR
        trace = pm.sample(
            500, tune=500, chains=1, cores=1, target_accept=0.9, random_seed=42, progressbar=True
        )


        posterior_pred = pm.sample_posterior_predictive(
            trace, var_names=["f_obs"], random_seed=42
        ) #here we are talkn predictive


    b_samples = trace.posterior["b"].values.flatten()
    median_b = np.median(b_samples) #MEDIAN BASE F


    gen_notes = posterior_pred.posterior_predictive["f_obs"].values
    n_samples = gen_notes.shape[0] * gen_notes.shape[1]
    gen_notes = gen_notes.reshape(n_samples, gen_notes.shape[2])

    #COMPUTE MSE FOR OBSERVED AND OTHERS
    mse_obs = np.mean((f_obs - np.median(gen_notes, axis=0))**2)

    #this is for the p value or an equivalent
    mse_samples = np.mean((gen_notes - f_obs[None, :])**2, axis=1)
    bayes_p_value = np.mean(mse_samples <= mse_obs)


    is_harmonic = bayes_p_value < 0.05


    print(f"Median inferred base frequency (b): {median_b:.2f} Hz")
    print(f"Mean squared error of fit: {mse_obs:.2f}")
    print(f"Bayesian posterior predictive p-value: {bayes_p_value:.4f}")
    print("V Harmonic!" if is_harmonic else "X Not harmonic.")



    results[f'Individual_{i}'] = {
        "median_b": median_b,
        "mse": mse_obs,
        "bayes_p_value": bayes_p_value,
        "is_harmonic": is_harmonic
    }

"""# PLOT HIST ANNNND THE LINES OF HARMONICITY :/"""

f0s

plt.hist(f0s, bins=np.arange(180, 520, 20), alpha=0.6, color='green', label='Notes') #distribution of f0s

max_harmonics = 25
harmonics = [best_f0 * h for h in range(1, max_harmonics + 1) if best_f0 * h <= 2000]
for first, h_freq in enumerate(harmonics):
    if first == 0 :
      plt.axvline(h_freq, color='red', linestyle='--', linewidth=1, label = "Harmonic Frequencies")
    else: #sinon, ça va plot toutes les lignes which we do not want !
      plt.axvline(h_freq, color='red', linestyle='--', linewidth=1)

plt.title("Distribution of f0 values in vocalizations' notes with computed Harmonic frequencies (Hz)")
plt.xlabel("F0 (Hz)")
plt.ylabel("Count")
plt.legend()
plt.grid(True)
plt.tight_layout()
# plt.savefig("/content/drive/MyDrive/gibbons/histogram_with_harmonicsIND1NGG.png", dpi=300)
plt.show()

"""## CONSONANCE TEST

ok, serious here :
from the  two doolittle articles (unisson article abt hupman music and the bird music one) I selected these intervals there are 11 in total :



- Unisson : 1/1
- Minor second: 16/15
- Major second : 9/8
- Minor third : 6/5
- Major third : 5/4
- Perfect fourth : 4/3
- Tritone : 45/32
- Perfect fifth : 3/2
- Minor sixth : 8/5
- Major sixth : 5/3
- Octave : 2/1
"""

f0s

from math import log2
All_intervals = { "Unisson" : 1/1, "Minor second" : 16/15, "Major second" : 9/8, "Minor third" : 6/5, "Major third" : 5/4, "Perfect fourth" : 4/3,"Tritone" : 45/32, "Perfect fifth" : 3/2, "Minor sixth" : 8/5, "Major sixth" : 5/3, "Octave" : 2/1, }
def consonant_intervals(f0, tolerance = 0.05) : #tolerance is compl arbitrary
  if len(f0) <2 :
    return []
  results = []
  #through each pair of successive notes
  for i in range(len(f0) - 1) :
    previous = f0[i]
    next = f0[i+1]

    ratio = previous / next
    cents = 1200 * log2 (ratio) #unit of semi tone and tone in music
#find closest matching interval from our LIST ABOVE ALL INTERVALS
    interval_name = "Other"
    for name, ideal_ratio in All_intervals.items():
      if np.isclose(ratio, ideal_ratio,rtol=tolerance) :
        interval_name = name
        break
    results.append({
        "previous_note" : previous,
        "next note" : next,
        "interval ratio" : ratio,
        "interval_cents" : cents,
        "consonant_interval_name" : interval_name
      })
  return pd.DataFrame(results)

resultats = consonant_intervals(f0s)
resultats.to_csv("/content/drive/MyDrive/gibbons/Dena/YELLO/consonance_resultsyellow3.csv", index=False)

resultats # YEEEEEEEEEEEEEEEEEEEEEEEEEY

"""# DISTRIBUTION OF INTERVALS FOR INVIV 1"""

# Reference intervals plus 'Other'
allintervallss = {
    "Unisson": 1/1,
    "Minor second": 16/15,
    "Major second": 9/8,
    "Minor third": 6/5,
    "Major third": 5/4,
    "Perfect fourth": 4/3,
    "Tritone": 45/32,
    "Perfect fifth": 3/2,
    "Minor sixth": 8/5,
    "Major sixth": 5/3,
    "Octave": 2/1,
    "Other": None
}

interval_names = list(allintervallss.keys())

individuals = ["Individual 1", "Individual 2", "Individual 3", "Individual 4", "Individual 5"]
files = [f"/content/drive/My Drive/gibbons/Dena/YELLO/consonance_resultsyellow{i}.csv" for i in range(1,6)]

distributions = {}
zero_counts_info = {}


for ind, file in zip(individuals, files):
    df = pd.read_csv(file)
    counts = df['consonant_interval_name'].value_counts()
    counts = counts.reindex(interval_names, fill_value=0)  # include all intervals + 'Other'
    distributions[ind] = counts
    zero_counts_info[ind] = counts[counts == 0].index.tolist()


all_dist = pd.DataFrame(distributions)


totals = all_dist.sum(axis=1)
sorted_intervals = totals.sort_values(ascending=False).index.tolist()
all_dist = all_dist.reindex(sorted_intervals)

ax = all_dist.plot(kind='bar', alpha=0.7)
plt.xlabel("Interval Name")
plt.ylabel("Count")
plt.title("Comparison of consonant interval distribution within \n Nomascus gabriellae \n (Ordered by the most present first)", fontsize=12)
plt.legend(title="Individuals")
plt.grid(True, axis='y')
plt.tight_layout()
plt.savefig("/content/drive/MyDrive/gibbons/Dena/YELLO/Distribution_consonant_intervals_all_with_zero_yellow_sorted.png", dpi=300)
plt.show()

distribution_consonant_intrv = resultats['consonant_interval_name'].value_counts()

distribution_consonant_intrv.plot(kind='bar', alpha=0.6, color='red', label='Consonant intevrals') #distribution of f0s

plt.title("Distribution of consonant intervals for invidivual 1 SGG")
plt.xlabel("Inetrval Name")
plt.ylabel("Count")
plt.legend()
plt.grid(True)
plt.tight_layout()
# plt.savefig("/content/drive/MyDrive/gibbons/Distribution of consonant intervals for invidivual 1 SGG.png", dpi=300)
plt.show()

"""# IT ENDS HERE, all the bellow is additional personal !

# TEST statistically the relevance of these intervals ?

My data is categories not cotinous data, so OLS won't work here. I can not use Chi squared goodness of fit tesT bc I have no expectations of what to expect. Except if I CHOOSE to test for pure randomness hypothesis...

therefore, maybe permutation (randomization) test ????



 Let's try the chi test first !
"""

f0s

from scipy.stats import chisquare
import numpy as np
def random_dist_chitest (f0s, f0_start, f0_end, trials=1000) :
  random_int = []
  for x in range (trials):
    random_f0s = np.random.uniform(f0_start, f0_end, len(f0s))
    random_results = consonant_intervals(random_f0s)
    random_int.extend(random_results['consonant_interval_name'].tolist())

  random_distribution = pd.Series(random_int).value_counts()
  return random_distribution
min_t =300
max_t =1700
expected_pro = random_dist_chitest(f0s, min_t, max_t, trials=1000) # le truc attendu

total_observations = len(resultats)
expected_pro = expected_pro / expected_pro.sum()
expected_counts = expected_pro * total_observations


observed_distribution = resultats["consonant_interval_name"].value_counts()
combined_index = observed_distribution.index.union(expected_pro.index)
observed_counts = observed_distribution.reindex(combined_index, fill_value=0)
expected_counts = expected_counts.reindex(combined_index, fill_value=0)

#test runn
chi_squared_statistic, p_value = chisquare(f_obs=observed_counts, f_exp=expected_counts)

print(f"p-value: {p_value}")

if p_value < 0.05 :
  print("significant, likely not random")
else :
  print("trop nul, it is random :(")

"""# SALIENCE is another parameter that IS NOT PART OF THE DESU PROJECT
#This part is personnal project part
"""

segment_outputs = []

for i, seg in tqdm(grey6.iterrows()):
    start_t = seg['Begin Time (s)']
    end_t = seg['End Time (s)']

    segment_data = grey6[(grey6ss['time'] >= start_t) & (grey6ss['time'] <= end_t)]

    if segment_data.empty:
        continue

    # Find the row with the maximum salience
    max_salience_idx = segment_data['salience'].idxmax()
    max_row = grey6ss.loc[max_salience_idx]

    segment_outputs.append({
        'segment': i,
        'start': start_t,
        'end': end_t,
        'max_salience_time': max_row['time'],
        'max_salience_value': max_row['salience'],
        'confidence_at_max_salience': max_row['confidence'],
        'f0_at_max_salience': max_row['f0']  # <–– Include the f0 value here
    })

# Convert to DataFrame and export
segment_df6 = pd.DataFrame(segment_outputs)
segment_df6.to_csv('/content/drive/MyDrive/segment_max_salience.csv', index=False)

segment_df1

from scipy.optimize import minimize_scalar
#extract the max energy
f0s = segment_df6['f0_at_max_salience'].dropna().values

# mse
def compute_log_mse(fi, f0s, max_harmonics=16):
    harmonics = np.array([fi * i for i in range(1, max_harmonics + 1)])
    mse = np.mean([np.min((f0 - harmonics)**2) for f0 in f0s])
    return mse


result = minimize_scalar(compute_log_mse, bounds=(20, 500), args=(f0s,), method='bounded')
best_f0 = result.x #results stored here
best_mse = result.fun


log_f0s = np.log(f0s)
mu, sigma = np.mean(log_f0s), np.std(log_f0s)

random_mses = []
for _ in range(1000):
    random_sample = np.clip(np.random.lognormal(mean=mu, sigma=sigma, size=len(f0s)), 0, 2000)
    res = minimize_scalar(compute_log_mse, bounds=(20, 500), args=(random_sample,), method='bounded')
    random_mses.append(res.fun)

p_value = np.mean(np.array(random_mses) <= best_mse)

print(f" Best base f₀ (harmonic fit): {best_f0:.2f} Hz")
print(f" p-value: {p_value:.4f}")
print("✅ Harmonic!" if p_value < 0.05 else "❌ Not harmonic.")

plt.hist(f0s, bins=np.arange(600, 2000, 20), alpha=0.6, color='green', label='max_energy_f0')

max_harmonics = 25
harmonics = [best_f0 * h for h in range(1, max_harmonics + 1) if best_f0 * h <= 2000]
for h_freq in harmonics:
    plt.axvline(h_freq, color='red', linestyle='--', linewidth=1)

plt.title("Histogram of max_energy_f0 with Harmonic freq Hz")
plt.xlabel("F0 (Hz)")
plt.ylabel("Count")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("/content/drive/MyDrive/gibbons/histogram_with_harmonicsgreyS.png", dpi=300)
plt.show()

"""#FIN DE SALIENCE ANALYSIS

Extract the energy max and min
"""

# #Making loops if possible but not for now.
# audiogrey = [audiogrey1, audiogrey2, audiogrey3, audiogrey4, audiogrey5, audiogrey6]
# greyf0 = [grey1f0, grey2f0, grey3f0, grey4f0, grey5f0, grey6f0]
# greys = [grey1s, grey2s, grey3s, grey4s, grey5s, grey6s]
# annotationgrey = [grey1, grey2, grey3, grey4, grey5, grey6]

#WARNING

#Load full audio
#grey1f0 = the f0s of individual (number = code)
#grey1 = has the annotation (time code and end for each segment of vocalization)

sig, fs = librosa.load(audiogrey2, sr=None)
f,t, Sxx = spectrogram(sig, fs=fs, nperseg=2048, noverlap=1500) #compute the spectrogram here
#convert to dB scale (energy)
Sxx_dB = 10 * np.log10(Sxx + 1e-10)

grey2f0['energy'] = np.nan #to create an empty column

#extract energy at each f0 point yeey
for idx, row in grey2f0.iterrows():
  time_point = row['time']
  freq_point = row['f0']

  idx_col = np.argmin(np.abs(t - time_point))
  idx_row = np.argmin(np.abs(f - freq_point))

  row_start = max(idx_row - 5, 0)
  row_end = min(idx_row + 5, Sxx_dB.shape[0])

  grey2f0.loc[idx, 'energy'] = np.max(Sxx_dB[row_start:row_end, idx_col])

grey1

#WARNING

# Histogram
plt.hist(
    segment_df["max_energy_f0"],
    bins=np.arange(600, 2000, 20),
    alpha=0.5,
    color="green",
    label="max_energy_f0"
)

plt.xlabel('F0 (Hz)')
plt.ylabel('Count')
plt.title('Histogram of max_energy_f0')
plt.legend()
plt.tight_layout()
plt.show()

"""Results :
Indiv 1 : harmonic
Indiv 2 : non harmonic (weird selectED points)
Indiv 3 : not harmonic (weird selectED points) :')
Indiv 4 : HARMONIC !!!!!
Invid 5 : NOT HARMONIC (0.07 p value !!) POINTS ON POINT !
Indiv 6 : NOT HARMONIC AT ALL

NOTE : DIFF distributions en terme de freq !
"""

plt.hist(f0s, bins=np.arange(600, 2000, 20), alpha=0.6, color='green', label='max_energy_f0')

max_harmonics = 25
harmonics = [best_f0 * h for h in range(1, max_harmonics + 1) if best_f0 * h <= 2000]
for h_freq in harmonics:
    plt.axvline(h_freq, color='red', linestyle='--', linewidth=1)

plt.title("Histogram of max_energy_f0 with Harmonic freq Hz")
plt.xlabel("F0 (Hz)")
plt.ylabel("Count")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("/content/drive/MyDrive/gibbons/histogram_with_harmonicsgrey3.png", dpi=300)
plt.show()

# Load audio and compute spectrogram
sig, fs = librosa.load(audiogrey2, sr=None)
f, t, Sxx = spectrogram(sig, fs=fs, nperseg=2048, noverlap=1500)

# Convert power spectrogram to dB
Sxx_dB = 10 * np.log10(Sxx + 1e-10)

# idx
segment_index = 50
segment = segment_df2.loc[segment_df2['segment'] == segment_index].iloc[0]

start_t = segment['start']
end_t = segment['end']

# indice time window
time_mask = (t >= start_t) & (t <= end_t)

# spect plot seg 1 ONLYYYY
plt.figure(figsize=(12, 6))
plt.pcolormesh(t[time_mask], f, Sxx_dB[:, time_mask], shading='gouraud', cmap='magma')
plt.colorbar(label='Intensity [dB]')
plt.ylabel('Frequency [Hz]')
plt.xlabel('Time [s]')
plt.title(f'Spectrogram segment {segment_index} from {start_t:.2f}s to {end_t:.2f}s')

# Overlay max energy frequency point POIIIIIIIIIIINT YEEEY
plt.plot(segment['max_energy_time'], segment['max_energy_f0'], 'bo', label='Max Energy F0')

#plt.plot(segment['max_freq_time'], segment['max_freq'], 'go', label='Max Freq')
#plt.plot(segment['min_freq_time'], segment['min_freq'], 'ro', label='Min Freq')

plt.legend()
plt.ylim(500, 1000)
plt.show()